\section{Clustering Analysis by K-means} % (fold)
\label{sec:clustering_analysis_by_k_means}
In questa prima sezione del capitolo relativo al Clustering, ci occupiamo di rilevare ,attraverso l'applicazione
dell'algoritmo \textit{K-means}, i gruppi di dipendenti con caratteristiche comuni all'interno del data set.
La nostra analisi è stata inizialmente svolta sul data set completo, utilizzando però solo alcune variabili tra
quelle a disposizione, come verrà spiegato nelle sezioni successive. Per questa analisi globale vengono
inoltre fornite alcune statistiche, per meglio integrare le informazioni ottenute dal clustering. Come ulteriori
casi di studio, sono riportate anche le analisi relative alla sola porzione del data set contenente gli impiegati
che hanno lasciato l'azienda e a quella invece contenente gli impiegati ancora al suo interno.
\subsection{Choice of attributes and distance function} % (fold)
\label{sub:choice_of_attributes_and_distance_function}
\begin{figure}[b!]
  \centering
  \includegraphics[width=\textwidth, height=10cm]{images/kmeans/SSE.pdf}
  \caption{Prova}
  \label{fig:sse}
\end{figure}
Le variabili sulle quali abbiamo deciso di applicare la Cluster Analysis tramite K-means sono le due
variabili di tipo continous presenti nel Dataset, ossia \textit{Satisfaction Level} e \textit{Latest Evaluation},
e le variabili \textit{Number Project}, \textit{Average Montly Hours} e \textit{Time Spend Company}, di tipo
discreto. Sono state escluse le variabili binarie. La distance function da noi utilizzata per rappresentare la
distanza tra due data objects è la distanza Euclidea. Come già specificato nella Sezione
\ref{sec:variable_transformations}, i valori delle variabili discrete sono stati normalizzati in un intervallo
compreso tra $0$ e $1$, al fine di rendere più agevole il confronto in fase di clustering. Inoltre, per ottenere
un risultato più pulito sono stati rimossi gli outliers ($1282$ records) presenti nella variabile
\textit{Time Spend Company}, come evidenziato nella Sezione \ref{sec:assessing_data_quality}.
% subsection choice_of_attributes_and_distance_function (end)
\subsection{Identification of the best value of k} % (fold)
\label{sub:identification_of_the_best_value_of_k}
Al fine di identificare il miglior numero $k$ di clusters da utilizzare, abbiamo tenuto conto dell'Error Sum of
Squares (SSE) per ogni iterazione dell'algoritmo, svolta a partire da un valore iniziale di $k$ pari a $2$ fino
ad un valore massimo di $50$. Rappresentato in Figura \ref{fig:sse} troviamo l'andamento dell'SSE per i
cluster relativi ai dipendenti che hanno lasciato l'azienda.
% subsection identification_of_the_best_value_of_k (end)
\subsection{Characterization of the obtained clusters } % (fold)
\label{sub:characterization_of_the_obtained_clusters}
Andiamo adesso a dare una descrizione dei cluster emersi dall'applicazione dell'algoritmo K-means con i parametri
che abbiamo deciso di utilizzare. Cominciando con l'osservare lo scatterplot relativo ai cluster degli impiegati
che hanno lasciato l'azienda, Figura , notiamo che sono ben visibili $3$ categorie distinte di impiegati, che
possiamo descrivere discorsivamente come
\begin{itemize}
    \item Impiegati con un livello di soddisfazione basso e un alto score nella valutazione. Questo cluster ci
    suggerisce la motivazione per cui questi impiegati hanno lasciato l'azienda, ossia una quantità troppo elevata
    di ore di lavoro, dalla quale deriva probabilmente l'alto score nella valutazione.
    \item Impiegati con un basso livello di soddisfazione, compreso tra $0.3$ e $0.5$, e un altrettanto basso score
    nella valutazione, compreso tra $0.2$ e $0.55$. La scarsa produttività e soddisfazione suggeriscono che questo
    gruppo di impiegati ha deciso di lasciare l'azienda per insoddisfazione verso la posizione lavorativa offerta.
    \item Impiegati con un alto livello di soddisfazione e un alto score nella valutazione, che probabilmente hanno
    lasciato l'azienda dopo aver ricevuto un'offerta di lavoro più vantaggiosa.
\end{itemize}
\lipsum
\begin{figure}[t]
    \begin{center}
        \subcaptionbox{\label{fig:cluster_left}}{\includegraphics[scale=0.45]{images/kmeans/cluster_left.pdf}}
        \subcaptionbox{\label{fig:cluster_stayed}}{\includegraphics[scale=0.45]{images/kmeans/cluster_stayed.pdf}}
        \caption{Nella Figura \ref{fig:cluster_left} troviamo i cluster emersi dall'analisi dei dipendenti che
        hanno lasciato l'azienda, nella Figura \ref{fig:cluster_stayed} troviamo invece quelli emersi dall'analisi
        dei dipendenti rimasti nell'azienda.}
    \end{center}
\end{figure}
% subsection characterization_of_the_obtained_clusters (end)
% section clustering_analysis_by_k_means (end)


\newpage
\section{Hierarchical clustering}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_euclidean.pdf}
  \caption{Dendrogramma per method X e metrica Y}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/methods_comparison.pdf}
  \caption{Confronto tra diversi metodi}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{../images/hierarchical/silhouette_average_euclidean_n2.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{../images/hierarchical/silhouette_comparison.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}


