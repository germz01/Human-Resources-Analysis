Al fine di costruire un \textit{decision tree} con i record a nostra disposizione, abbiamo eseguito alcune
manipolazioni prima della costruzione dell'albero vero e proprio: sia la variabile \textit{Salary} che la
variabile \textit{Department} sono state convertite in variabili numeriche, per facilitare l'analisi
dei record, inoltre, per fornire un \textit{target} durante la costruzione dell'albero, la
variabile
\textit{Left} è stata separata dal dataset fornito. Tutte le altre variabili sono state lasciate inalterate.
\section{Learning of different decision trees} % (fold)
\label{sec:learning_of_different_decision_trees}
Durante la fase di costruzione vera e propia, abbiamo deciso di implementare due alberi distinti,
differenziandoli
in base alla metrica con sui sarebbe stato scelto lo \textit{split} migliore. Abbiamo perciò costruito un albero
basato sull'indice \textit{Gini} come misura di impurità di un nodo, e un albero basato invece sull'uso
dell'\textit{Entropia}. Utilizzando questi due indici di impurità, al momento di decidere quale split
imporre, viene selezionato quello con il \textit{gain} maggiore. Inizialmente abbiamo costruito entrambi gli
alberi senza dare un limite sul numero di samples che un nodo avrebbe dovuto contenere per poter essere
diviso, successivamente però, per evitare di incorrere nel noto fenomeno dell'\textit{overfitting}, abbiamo
voluto esplorare le possibilità date dal consentire uno split a patto che almeno il $10\%$ o il $20\%$ del numero
totale di data objects sia presente nel nodo da dividere. In questa fase iniziale abbiamo utilizzato l'intero
dataset come base per il training dell'albero, nelle sezioni successive modificheremo questa assunzione,
dividento i data objects forniti tra test set e training set in percentuali ben definite.
% section learning_of_different_decision_trees (end)
\section{Decision trees interpretation} % (fold)
\label{sec:decision_trees_interpretation}
Facciamo adesso alcune considerazioni sugli alberi ottenuti applicando i parametri descritti in Sezione
\ref{sec:learning_of_different_decision_trees}. Sia per l'albero
basato sull'indice {} che per quello basato sull'Entropia, la radice è rappresentata dalla variabile
\textit{Satisfaction Level}, che divide i data objects tra coloro che hanno uno score $\le 0.465$ e coloro che
hanno uno score $> 0.465$. Questa decisione è indicativa del fatto che l'utilizzo di tale variabile consente un
primo split più redditizio rispetto alle altre variabili a disposizione. Come era lecito aspettarsi, gli alberi
ottenuti, non imponendo un vincolo sul numero di samples in ogni nodo prima dello split, hanno un'altezza
considerevole, tale altezza decresce in modo significativo applicando i vincoli esposti in Sezione
\ref{sec:learning_of_different_decision_trees}, arrivando ad ottenere un albero di altezza $13$ per l'indice Gini
e uno di altezza $11$ per l'Entropia. In Figura \ref{fig:cutted_tree} possiamo osservare i dettagli riguardanti
le radici degli alberi di altezza $11$, a destra, e $13$, a sinistra. Al fine di ottenere questi alberi è stato
limitato il numero di records minimo contenuto all'interno di un nodo al $20\%$ del totale a disposizone nel
dataset.
\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{
        \includegraphics{images/classification/decision_tree_gini_0_2_cutted_2.pdf}
        }
        \caption{}
        \label{fig:cutted_tree_1}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{
            \includegraphics{images/classification/decision_tree_entropy_0_2_cutted_2.pdf}
        }
        \caption{}
        \label{fig:cutted_tree_2}
    \end{subfigure}
    \caption{Dettaglio iniziale dell'albero di altezza $13$ utilizzante Gini, in Figura \ref{fig:cutted_tree_1}
    , e quello di altezza $11$ utilizzante l'Entropia, in Figura \ref{fig:cutted_tree_2}. La classe \textit{y$0$}
    corrisponde a una risposta negativa alla domanda: "L'impiegato lascerà l'azienda?", vale il contrario per la
    classe y$1$. Essendo l'intero albero di dimensioni proibitive, abbiamo deciso di rappresentare questo
    dettaglio.}
    \label{fig:cutted_tree}
\end{figure}
\\
Come possiamo osservare, la colorazione dei vari nodi fornisce un indice visivo del grado di purezza dello split
ottenuto, cominciando da uno split iniziale su tutto il dataset con un grado di purezza non molto alto
(nel caso dell'indice Gini), si raggiunge successivamene negli split secondari, su porzioni ridotte del dataset,
gradi di purezza più consistenti.
% section decision_trees_interpretation (end)
\section{Decision trees validation with test and training set} % (fold)
\label{sec:decision_trees_validation_with_test_and_training_set}
Prendiamo quanto trattato nelle sezioni precedenti e passiamo adesso a descrivere l'applicazione dei
classificatori da noi costruiti. In Tabella \ref{tab:classification_performance} sono riportate le performance
dei migliori $5$ set di impostazioni per i classificatori. Come possiamo vedere dalla tabella, la nostra analisi è
stata condotta prima prendendo l'intero dataset sia come training set che come test set, poi prendendone l'$80\%$
come training set e il restante $20\%$ come test set e così via, modificando le percentuali fino ad utilizzare il
$60\%$ del dataset come training set e la restante parte come test set. La valutazione delle performance è stata
condotta analizzando lo score ottenuto utilizzando le metriche \textit{Accuracy}, \textit{Precision} e
\textit{Recall}, apprese durante il corso.
\begin{table}
    \centering
    \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{| c | c | c | c | c | c | c |}
            \hline
            \textbf{Criterion} & \textbf{Min Sample Split} & \textbf{Train Records} & \textbf{Test Records} &
            \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\ \hline
            Entropy & 2 & 14999 (100\%) & 14999 (100\%) & 1.0 & 0.99 & 0.99 \\ \hline
            Gini & 2 & 14999 (100\%) & 14999 (100\%) & 0.99 & 0.99 & 0.98 \\ \hline
            Entropy & 2 & 8999 (60\%) & 6000 (40\%) & 0.98 & 0.96 & 0.94 \\ \hline
            Gini & 2 & 11999 (80\%) & 3000 (20\%) & 0.98 & 0.95 & 0.95 \\ \hline
            Gini & 2 & 10499 (70\%) & 4500 (30\%) & 0.98 & 0.95 & 0.95 \\ \hline
%             \textbf{Gini} & 2.0 & 14999 & 14999 & 0.99 & 0.99 & 0.98 \\ \hline
%             {} & {} & 11999 & 3000 & 0.98 & 0.95 & 0.95 \\ \hline
%             {} & {} & 10499 & 4500 & 0.98 & 0.95 & 0.95 \\ \hline
%             {} & {} &  8999 & 6000 &  0.98 & 0.95 & 0.94 \\ \hline
%             {} &  0.1 & 14999 & 14999 & 0.96 & 0.90 & 0.93 \\ \hline
%       {} &               {} &      11999 &      3000 &      0.96 &       0.90 &    0.94 \\ \hline
%       {} &               {} &      10499 &      4500 &      0.96 &       0.90 &    0.93 \\ \hline
%       {} &               {} &       8999 &      6000 &      0.95 &       0.89 &    0.88 \\ \hline
%       {} &               0.2 &      14999 &     14999 &      0.85 &       0.88 &    0.43 \\ \hline
%       {} &               {} &      11999 &      3000 &      0.86 &       0.91 &    0.44 \\ \hline
%       {} &               {} &      10499 &      4500 &      0.86 &       0.89 &    0.44 \\ \hline
%       {} &               {} &       8999 &      6000 &      0.87 &       0.74 &    0.65 \\ \hline
%    \textbf{Entropy} &               2.0 &      14999 &     14999 &      1.00 &       0.99 &    0.99 \\ \hline
%    {} &               {} &      11999 &      3000 &      0.98 &       0.95 &    0.95 \\ \hline
%    {} &               {} &      10499 &      4500 &      0.98 &       0.95 &    0.95 \\ \hline
%    {} &               {} &       8999 &      6000 &      0.98 &       0.96 &    0.94 \\ \hline
%    {} &               0.1 &      14999 &     14999 &      0.96 &       0.90 &    0.93 \\ \hline
%    {} &               {} &      11999 &      3000 &      0.96 &       0.90 &    0.94 \\ \hline
%    {} &               {} &      10499 &      4500 &      0.96 &       0.90 &    0.93 \\ \hline
%    {} &               {} &       8999 &      6000 &      0.96 &       0.90 &    0.92 \\ \hline
%    {} &               0.2 &      14999 &     14999 &      0.91 &       0.92 &    0.68 \\ \hline
%    {} &               {} &      11999 &      3000 &      0.92 &       0.94 &    0.69 \\ \hline
%    {} &               {} &      10499 &      4500 &      0.91 &       0.92 &    0.68 \\ \hline
% {} &               {} &       8999 &      6000 &      0.91 &       0.92 &    0.67 \\ \hline
        \end{tabular}
    }
    \caption{Tabella contentente le performance relative alle metriche di valutazione utilizzate per la
    classificazione tramite indice Gini ed Entropia. Min Sample Split rappresenta la minima percentuale di record
    rispetto al dataset totale che deve essere contenuta in ogni nodo (il valore $2$ rappresenta il fatto che
    almeno $2$ nodi devono essere presenti all'interno del nodo).}
    \label{tab:classification_performance}
\end{table}
% section decision_trees_validation_with_test_and_training_set (end)
\section{Discussion of the best prediction model} % (fold)
\label{sec:discussion_of_the_best_prediction_model}
Descriviamo adesso i risultati dei migliori classificatori ottenuti in quest'ultimo passo del report. Come era
lecito aspettarsi, i primi due classificatori per score ottenuto nelle metriche utilizzate sono quelli costruiti
utilizzando tutto il dataset come training set e successivamente come test set. Successivamente, il classificatore
utilizzante l'Entropia come misura di impurità ha rivelato delle performance migliori rispetto a quello
utilizzante Gini. In Figura \ref{fig:confusion_matrix} possiamo trovare la \textit{confusion matrix} relativa a
tale classificatore. Come possiamo vedere, la sezione relativa ai \textit{true negatives} contiene il massimo dei
records, come evidenziato dal colore, i \textit{false negatives} e i \textit{false positives} sono le due sezioni
contenenti il numero minimo (tendente a $0$) di records, facilmente prevedibile viste le alte performance di
questo classificatore. La parte interessante della figura è rappresentata da quella dei \textit{true positives}.
Visto che il campione sulla quale è stato applicato questo classificatore è di $6000$ records, e che tra questi
data objects sono presenti per lo più impiegati che lavorano ancora all'interno dell'azienda, possiamo concludere
che coloro che hanno lasciato l'azienda sono stati correttamente previsti dal classificatore. A scopo informativo,
in Figura \ref{fig:confusion_matrix_2} abbiamo riportato la confusion matrix relativa alla quarta riga di Tabella
\ref{tab:classification_performance}, che, a differenza dell'altra matrice descritta, utilizza Gini come indice
di impurità. In quest'ultimo classificatore, applicato su $3000$ test records, possiamo osservare un incremento
nella parte della matrice relativa ai false negatives.
\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{
            \includegraphics[width=\textwidth]{images/classification/confusion_matrices/entropy_confusion_matrix_0_4_test.pdf}
        }
        \caption{}
        \label{fig:confusion_matrix}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{
            \includegraphics[width=\textwidth]{images/classification/confusion_matrices/gini_confusion_matrix_0_2_test.pdf}
        }
        \caption{}
        \label{fig:confusion_matrix_2}
    \end{subfigure}
    \caption{In Figura \ref{fig:confusion_matrix} troviamo la confusion matrix relativa alla terza riga di
    Tabella \ref{tab:classification_performance}, mentre in Figura \ref{fig:confusion_matrix_2} troviamo quella
    relativa alla quarta riga di Tabella \ref{tab:classification_performance}.}
    \label{fig:confusion_matrices}
\end{figure}
% section discussion_of_the_best_prediction_model (end)
