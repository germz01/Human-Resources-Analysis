In questa secondo capitolo del report, ci occupiamo di rilevare ,attraverso l'applicazione
degli algoritmi \textit{K-means}, \textit{Hierarchical} e \textit{Dbscan}, i gruppi di dipendenti con
caratteristiche comuni all'interno del data set. La nostra analisi è stata inizialmente svolta sul data set
completo, utilizzando però solo alcune variabili tra quelle a disposizione, come verrà spiegato nella sezione
\ref{sub:choice_of_attributes_and_distance_function}. Per questa analisi globale vengono inoltre fornite alcune
statistiche, per meglio integrare le informazioni ottenute dal clustering. Come già specificato nella Sezione
\ref{sec:variable_transformations}, i valori delle variabili discrete sono stati normalizzati in un intervallo
compreso tra $0$ e $1$, al fine di rendere più agevole il confronto in fase di clustering. Infine, come ulteriori
casi di studio, sono riportate anche le analisi relative alla sola porzione del data set contenente gli impiegati
che hanno lasciato l'azienda e a quella invece contenente gli impiegati ancora al suo interno.
\section{Clustering Analysis by K-means} % (fold)
\label{sec:clustering_analysis_by_k_means}
\subsection{Choice of attributes and distance function} % (fold)
\label{sub:choice_of_attributes_and_distance_function}
Le \textit{variabili} sulle quali abbiamo deciso di applicare l'analisi tramite K-means sono un sottoinsieme di
quelle a disposizione nel data set, in particolare, sono le variabili di tipo continuo
\textit{Satisfaction Level} e \textit{Latest Evaluation}, e le variabili di tipo discreto
\textit{Number Project}, \textit{Average Montly Hours} e \textit{Time Spend Company}. Sono state escluse le
variabili binarie. Vista la natura delle variabili utilizzate nell'applicazione dell'algoritmo, la distance
function da noi utilizzata per quantificare la distanza tra due data objects è la \textit{distanza Euclidea},
rappresentata dalla nota formula
\begin{equation*}
    dist(p,\ q) \ = \ \sqrt{\sum_{i = 0}^{k}(q_i - p_i)^2}\quad,
\end{equation*}
dove con $q_i$ e $p_i$ vengono rappresentati li i-esimi attributi dei data objects $p$ e $q$.
% subsection choice_of_attributes_and_distance_function (end)
\subsection{Identification of the best value of k} % (fold)
\label{sub:identification_of_the_best_value_of_k}
Al fine di identificare il miglior numero $k$ di clusters da utilizzare, abbiamo tenuto conto dell'
\textit{Error Sum of Squares} (SSE), ossia della somma, elevata al quadrato, della distanza tra ogni singolo data object e il centroide più vicino, ottenuta mediante la formula
\begin{equation*}
    SSE \ = \ \sum_{i = 1}^{K} \sum_{x \in C_i} dist(c_i,\ x)^2 \quad ,
\end{equation*}
dove \textit{dist} rappresenta la distanza Euclidea tra il centroide $c_i$ ed il data object $x$.
 A partire da un valore iniziale di $k$ pari a $2$ fino ad un valore
massimo di $50$ abbiamo calcolato l'SSE risultante dall'applicazione dell'algoritmo come possiamo osservare in
Figura \ref{fig:sse}, e abbiamo infine deciso per un valore di $k$ pari a $4$ per l'applicazione di K-means sul
data set totale. Tale valore è stato poi confermato come il più elevato tra quelli osservati tramite lo studio
del \textit{Silhouette score}.
% subsection identification_of_the_best_value_of_k (end)
\subsection{Characterization of the obtained clusters } % (fold)
\label{sub:characterization_of_the_obtained_clusters}
In quest'ultima sezione relativa all'algoritmo K-means descriviamo i clusters emersi durante l'analisi.
Utilizzando i parametri descritti nelle sezioni precedenti, abbiamo ottenuto i clusters raffigurati in Figura
\ref{fig:dist_clu}, dove possiamo osservare la densità di popolazione per ognuno dei cluster ottenuti.
In Tabella \ref{tab:stat_descr} abbiamo riportato i dati caratteristici di ognuno dei cluster scoperti.
Il primo cluster emerso, Cluster $0$, è quello con una densità di popolazione minore, e contraddistingue gli
impiegati con un alto score nelle variabili Average Montly Hours, Last Evaluation, Number Project e Time Spend
Company. Possiamo pensare a un tale cluster come a un gruppo di impiegati molto produttivi, ma con un livello di
soddisfazione non troppo elevato. Il secondo Cluster, Cluster $1$, presenta un gruppo di impiegati con un valore
molto basso sia in Time Spend Company che in Average Montly Hour, e valori poco sopra $0.55$ nelle altre variabili.
Il Cluster $2$ è probabilmente quello più allarmante, dal momento che è composto da impiegati con un bassissimo
score nella variabile Satisfaction Level, e uno score elevato sia in Last Evaluation che in Number Project.
Possiamo pensare a un gruppo di impiegati che hanno mantenuto un livello di produttività alto dovuto ad elevati
carichi di lavoro, fattore che ha comportato il calo vertiginoso del livello di soddisfazione. Il fatto che tale
cluster sia anche quello più densamente popolato è un ulteriore motivo di interesse verso questo gruppo di
impiegati. L'ultimo cluster, il $3$ presenta anch'esso un basso score nella variabile Satisfaction Level, mentre
vale il contrario per le variabili Last Evaluation e Number Project. Lo score di circa $0.41$ nella variabile
Time Spend Company fa pensare ad un gruppo di impiegati molto produttivi, i quali da non molto lavorano all'interno
dell'azienda, e a rischio dal momento che questo alto tasso di produttività sta probabilmente compromettendo il
loro livello di soddisfazione.
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.5\textwidth]{images/kmeans/dist_cluster.pdf}
  \caption{Distribuzione del numero di impiegati per ognuno dei cluster scoperti durante l'analisi, in
  ordine decrescente per densità di popolazione.\label{fig:dist_clu}}
\end{figure}
\begin{table}[H]
  \centering
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{| c | c | c | c | c | c |}
        \hline
        {} & \multicolumn{5}{c |}{Average\_Montly\_Hours} \\
        \hline
        {} & count & mean & std & min & max\\
        Cluster & & & & &  \\
        \hline
        0 & 1494.0 & 0.733762 & 0.204831 & 0.02 & 1.00 \\
        1 & 2987.0 & 0.251463 & 0.116789 & 0.00 & 0.88 \\
        2 & 6784.0 &  0.519805 &  0.194971 &  0.01 &  1.00 \\
        3 & 2452.0 &  0.542761 &  0.215429 &  0.00 & 0.99 \\
        \hline
      \end{tabular}
      }
      \caption{}
      \label{tab:uno}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{| c | c | c | c | c | c |}
        \hline
        {} & \multicolumn{5}{c |}{Last\_Evaluation} \\
        \hline
        {} & count & mean & std & min & max\\
        Cluster & & & & & \\
        \hline
        0 & 1494.0 &  0.811693 &  0.138791 &  0.37 & 1.0  \\
        1 & 2987.0 &  0.556739 &  0.114149 &  0.36 &  1.0  \\
        2 & 6784.0 &  0.737535 &  0.152870 &  0.36 &  1.0  \\
        3 & 2452.0 &  0.782822 &  0.168248 &  0.36 & 1.0 \\
        \hline
      \end{tabular}
      }
      \caption{}
      \label{tab:due}
  \end{subtable}
  \begin{subtable}{0.4\linewidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{| c | c | c | c | c | c |}
        \hline
        {} & \multicolumn{5}{c |}{Number\_Project} \\
        \hline
        {} & count & mean & std & min & max\\
        Cluster & & & & & \\
        \hline
        0 & 1494.0 &  0.811693 &  0.138791 &  0.37 & 1.0  \\
        1 & 2987.0 &  0.556739 &  0.114149 &  0.36 & 1.0  \\
        2 & 6784.0 &  0.737535 &  0.152870 &  0.36 & 1.0  \\
        3 & 2452.0 &  0.782822 &  0.168248 &  0.36 & 1.0 \\
        \hline
      \end{tabular}
    }
    \caption{}
    \label{tab:tre}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{| c | c | c | c | c | c |}
        \hline
        {} & \multicolumn{5}{c |}{Satisfaction\_Level} \\
        \hline
        {} & count & mean & std & min & max\\
        Cluster & & & & & \\
        \hline
        0       &  1494.0 &  0.633199 &  0.482093 &  0.0 &  1.0  \\
        1       &  2987.0 &  0.520589 &  0.499660 &  0.0 &  1.0   \\
        2       &  6784.0 &  0.012529 &  0.111240 &  0.0 &  1.0 \\
        3       &  2452.0 &  0.316476 &  0.465196 &  0.0 &  1.0  \\
        \hline
      \end{tabular}
    }
    \caption{}
    \label{tab:quattro}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{| c | c | c | c | c | c |}
        \hline
        {} & \multicolumn{5}{c |}{Time\_Spend\_Company} \\
        \hline
        {} & count & mean & std & min & max\\
        Cluster & & & & & \\
        \hline
        0 &         1494.0 &  0.753414 &  0.179673 &  0.0 &  1.0  \\
        1 &         2987.0 &  0.091530 &  0.141061 &  0.0 &  0.8   \\
        2  &         6784.0 &  0.364092 &  0.169420 &  0.0 &  1.0  \\
        3   &        2452.0 &  0.417129 &  0.193405 &  0.0 &  0.8   \\
        \hline
      \end{tabular}
    }
    \caption{}
    \label{tab:cinque}
  \end{subtable}
  \caption{Statistica descrittiva relativa alle variabili utilizzate nell'applicazione dell'algoritmo K-means
  in fase di clustering.}
  \label{tab:stat_descr}
\end{table}
% Andiamo adesso a dare una descrizione dei cluster emersi dall'applicazione dell'algoritmo K-means con i parametri
% che abbiamo deciso di utilizzare. Cominciando con l'osservare lo scatterplot relativo ai cluster degli impiegati
% che hanno lasciato l'azienda, Figura , notiamo che sono ben visibili $3$ categorie distinte di impiegati, che
% possiamo descrivere discorsivamente come
% \begin{itemize}
%     \item Impiegati con un livello di soddisfazione basso e un alto score nella valutazione. Questo cluster ci
%     suggerisce la motivazione per cui questi impiegati hanno lasciato l'azienda, ossia una quantità troppo elevata
%     di ore di lavoro, dalla quale deriva probabilmente l'alto score nella valutazione.
%     \item Impiegati con un basso livello di soddisfazione, compreso tra $0.3$ e $0.5$, e un altrettanto basso score
%     nella valutazione, compreso tra $0.2$ e $0.55$. La scarsa produttività e soddisfazione suggeriscono che questo
%     gruppo di impiegati ha deciso di lasciare l'azienda per insoddisfazione verso la posizione lavorativa offerta.
%     \item Impiegati con un alto livello di soddisfazione e un alto score nella valutazione, che probabilmente hanno
%     lasciato l'azienda dopo aver ricevuto un'offerta di lavoro più vantaggiosa.
% \end{itemize}
% \begin{figure}[t]
%     \begin{center}
%         \subcaptionbox{\label{fig:cluster_left}}{\includegraphics[scale=0.45]{images/kmeans/cluster_left.pdf}}
%         \subcaptionbox{\label{fig:cluster_stayed}}{\includegraphics[scale=0.45]{images/kmeans/cluster_stayed.pdf}}
%         \caption{Nella Figura \ref{fig:cluster_left} troviamo i cluster emersi dall'analisi dei dipendenti che
%         hanno lasciato l'azienda, nella Figura \ref{fig:cluster_stayed} troviamo invece quelli emersi dall'analisi
%         dei dipendenti rimasti nell'azienda.}
%     \end{center}
% \end{figure}
% subsection characterization_of_the_obtained_clusters (end)
% section clustering_analysis_by_k_means (end)


\newpage
\section{Hierarchical clustering}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_euclidean.pdf}
  \caption{Dendrogramma per method X e metrica Y}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/methods_comparison.pdf}
  \caption{Confronto tra diversi metodi}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{../images/hierarchical/silhouette_average_euclidean_n2.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{../images/hierarchical/silhouette_comparison.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}


