La ricerca di gruppi di dipendenti con caratteristiche affini all'interno del dataset è stata eseguita utilizzando
differenti tecniche di clustering. Per eseguire l'analisi sono state selezionate solamente le $5$ variabili
numeriche in Tab. \ref{tab:variabili}, in modo da calcolare le distanze tra i dati in modo appropriato.
Come già specificato nella Sezione \ref{sec:variable_transformations}, i valori delle variabili discrete sono
stati normalizzati in un intervallo compreso tra $0$ e $1$, al fine di rendere più agevole il confronto in fase di
clustering.
\section{Clustering Analysis by K-means} % (fold)
\label{sec:clustering_analysis_by_k_means}
\subsection{Choice of attributes and distance function} % (fold)
\label{sub:choice_of_attributes_and_distance_function}
Vista la natura delle variabili utilizzate nell'applicazione dell'algoritmo, la distance
function da noi utilizzata per quantificare la distanza tra due data objects è la \textit{distanza Euclidea}.
% subsection choice_of_attributes_and_distance_function (end)
\subsection{Identification of the best value of k} % (fold)
\label{sub:identification_of_the_best_value_of_k}
Al fine di identificare il miglior numero $k$ di clusters da utilizzare, abbiamo tenuto conto dell'
\textit{Error Sum of Squares} (SSE), ossia della somma, elevata al quadrato, della distanza tra ogni singolo data
object e il centroide più vicino. A partire da un valore iniziale di $k$ pari a $2$ fino ad un valore
massimo di $50$ abbiamo calcolato l'SSE risultante dall'applicazione dell'algoritmo, come possiamo osservare in
Figura \ref{fig:sse}, dove troviamo la rappresentazione in scala ridotta a partire dal valore iniziale $2$ e
finale $20$. Abbiamo infine deciso per un valore di $k$ pari a $4$ per l'applicazione di K-means sul
data set totale, in quanto ritenuto il valore più efficiente ai fini della nostra analisi. Tale valore è stato poi
confermato come il più elevato tra quelli osservati tramite lo studio del \textit{Silhouette score},
totalizzando un punteggio pari a $0.57$.
% subsection identification_of_the_best_value_of_k (end)
\subsection{Characterization of the obtained clusters } % (fold)
\label{sub:characterization_of_the_obtained_clusters}
In quest'ultima sezione relativa all'algoritmo K-means descriviamo i clusters emersi durante l'analisi.
Utilizzando i parametri descritti nelle sezioni precedenti, abbiamo ottenuto i clusters raffigurati in Figura
\ref{fig:dist_clu}, dove possiamo osservare la densità di popolazione per ognuno dei cluster ottenuti.
In Tabella \ref{tab:stat_descr} abbiamo riportato i dati caratteristici di ognuno dei cluster scoperti.
Il primo cluster emerso, Cluster $0$ contraddistingue gli impiegati con un alto score nelle variabili
Last Evaluation e Satisfaction Level. Possiamo pensare a un tale cluster come a un gruppo di impiegati molto
soddisfatti e valutati positivamente, da poco assunti, visto lo score basso in Time Spend Company. Il secondo
Cluster, Cluster $1$, presenta un gruppo di impiegati con uno score basso nelle variabili Average Montly
Hours e Satisfaction Level. Tale cluster, essendo il terzo cluster per densità di popolazione, denota un risultato
interessante e sicuramente da tenere d'occhio per i futuri sviluppi dell'azienda.
Il Cluster $2$ presenta un gruppo di impiegati soddisfatti e produttivi, con carichi di lavoro abbastanza elevati.
L'ultimo cluster, il $3$, presenta una situazione preoccupante, visto gli score molto elevati degli impiegati
nelle variabili Average Montly Hours, Last Evaluation e Number Project, e il basso score nella variabile
Satisfaction Level. Vedendo simili risultati è facile immaginarsi un gruppo di impiegati da poco assunti,
caratterizzati da un alto tasso di ore di lavoro, il quale ha comportato il calo nel livello di soddisfazione,
nonostante l'ultima valutazione fosse stata più che positiva.
\begin{figure}[t!]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \resizebox{\textwidth}{!}{
      \includegraphics{images/kmeans/SSE.pdf}
    }
    \caption{Sviluppo dell'SSE in base all'aumentare del numero di clusters nell'applicazione dell'algoritmo
    K-means.}
    \label{fig:sse}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \resizebox{\textwidth}{!}{
      \includegraphics[width=0.5\textwidth]{images/kmeans/dist_cluster.pdf}
    }
    \caption{Distribuzione del numero di impiegati per ognuno dei cluster scoperti durante l'analisi, in ordine
    decrescente per densità di popolazione.}
    \label{fig:dist_clu}
  \end{subfigure}
\end{figure}
\begin{table}[H]
  \centering
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    {} & \multicolumn{5}{c |}{Average\_Montly\_Hours} \\
    \hline
    {} & count & mean & std & min & max \\
    Cluster & & & & &  \\
    \hline
    0 & 4718.0 & 0.34 & 0.12 & 0.0 & 0.57 \\
    1 & 3112.0 & 0.27 & 0.13 & 0.0 & 0.88 \\
    2 & 5343.0 & 0.69 & 0.10 & 0.32 & 1.0 \\
    3 & 1826.0 & 0.70 & 0.22 & 0.01 & 1.0 \\
    \hline
    \end{tabular}
  }
  \caption{}
  \label{tab:dist_Average_Montly_Hours}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    {} & \multicolumn{5}{c |}{Last\_Evaluation} \\
    \hline
    {} & count & mean & std & min & max \\
    Cluster & & & & &  \\
    \hline
    0 & 4718.0 & 0.73 & 0.16 & 0.36 & 1.0 \\
    1 & 3112.0 & 0.56 & 0.12 & 0.36 & 1.0 \\
    2 & 5343.0 & 0.77 & 0.16 & 0.36 & 1.0 \\
    3 & 1826.0 & 0.80 & 0.15 & 0.36 & 1.0 \\
    \hline
    \end{tabular}
  }
  \caption{}
  \label{tab:dist_Last_Evaluation}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    {} & \multicolumn{5}{c |}{Number\_Project} \\
    \hline
    {} & count & mean & std & min & max \\
    Cluster & & & & &  \\
    \hline
    0 & 4718.0 & 0.37 & 0.17 & 0.0 & 1.0 \\
    1 & 3112.0 & 0.087 & 0.13 & 0.0 & 0.8 \\
    2 & 5343.0 & 0.38 & 0.18 & 0.0 & 0.8 \\
    3 & 1826.0 & 0.73 & 0.19 & 0.0 & 1.0 \\
    \hline
    \end{tabular}
  }
  \caption{}
  \label{tab:dist_Number_Project}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    {} & \multicolumn{5}{c |}{Satisfaction\_Level} \\
    \hline
    {} & count & mean & std & min & max \\
    Cluster & & & & &  \\
    \hline
    0 & 4718.0 & 0.75 & 0.15 & 0.29 & 1.0 \\
    1 & 3112.0 & 0.43 & 0.11 & 0.09 & 0.96 \\
    2 & 5343.0 & 0.75 & 0.15 & 0.25 & 1.0 \\
    3 & 1826.0 & 0.17 & 0.10 & 0.09 & 0.63 \\
    \hline
    \end{tabular}
  }
  \caption{}
  \label{tab:dist_Satisfaction_Level}
  \end{subtable}
  \begin{subtable}{0.4\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    {} & \multicolumn{5}{c |}{Time\_Spend\_Company} \\
    \hline
    {} & count & mean & std & min & max \\
    Cluster & & & & &  \\
    \hline
    0 & 4718.0 & 0.14 & 0.15 & 0.0 & 1.0 \\
    1 & 3112.0 & 0.17 & 0.15 & 0.0 & 1.0 \\
    2 & 5343.0 & 0.22 & 0.21 & 0.0 & 1.0 \\
    3 & 1826.0 & 0.29 & 0.15 & 0.0 & 1.0 \\
    \hline
    \end{tabular}
  }
  \caption{}
  \label{tab:dist_Time_Spend_Company}
  \end{subtable}
  \caption{Statistica descrittiva relativa ad ognuno dei cluster scoperti. Per ogni cluster vengono riportate le
  informazioni relative alla densità di popolazione, alla media, alla deviazione standard e ai valori minimi e
  massimi delle variabili utilizzate.}
  \label{tab:stat_descr}
  \end{table}
\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
      \includegraphics{images/kmeans/cluster_left.pdf}
    }
    \caption{}
    \label{fig:cluster_left}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
      \includegraphics{images/kmeans/cluster_stayed.pdf}
    }
    \caption{}
    \label{fig:cluster_stayed}
  \end{subfigure}
  \caption{Visualizzazione relativa all'applicazione dell'algoritmo K-means sul data set diviso in funzione della
  variabile \textit{Left}. In Figura \ref{fig:cluster_left} è possibile osservare il clustering relativo agli
  impiegati che hanno lasciato l'azienda, mentre in Figura \ref{fig:cluster_stayed} troviamo il clustering
  relativo agli impiegati che sono rimasti. L'analisi dell'SSE e dello score della silhouette ha rivelato che,
  applicando l'algoritmo soltanto sulle variabili Satisfaction Level e Last Evaluation, il numero ideale di
  clusters è $3$ per gli impiegati che hanno lasciato l'azienda, e $5$ per gli altri.}
  \label{fig:cluster_splitted}
\end{figure}
\newpage
Come ulteriore esempio, in Figura \ref{fig:cluster_splitted} forniamo le visualizzazioni relative all'applicazione
di K-means, utilizzando le variabili Satisfaction Level e Last Evaluation, al data set diviso in base alla
variabile Left. Similmente a quanto fatto per l'algoritmo applicato all'intero data set, abbiamo prima studiato
l'SSE, e confrontato le nostre ipotesi con lo score fornito dall'analisi della silhouette. Come possiamo vedere
nella Figura \ref{fig:cluster_left},
i $3$ clusters emersi per gli impiegati che hanno lasciato l'azienda delineano un gruppo di impiegati con un basso
score sia in Satisfaction Level che in Last Evaluation, un gruppo con un alto score in Last Evaluation e un basso
score in Satisfaction Level e un gruppo con alto score in entrambe le variabili. Per gli impiegati ancora
all'interno dell'azienda, possiamo notare nella Figura \ref{fig:cluster_stayed} che la situazione è decisamente
più distribuita.
\section{Hierarchical clustering}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_euclidean.pdf}
  \caption{Dendrogramma per method X e metrica Y}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/methods_comparison.pdf}
  \caption{Confronto tra diversi metodi}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{../images/hierarchical/silhouette_average_euclidean_n2.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{../images/hierarchical/silhouette_comparison.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}


