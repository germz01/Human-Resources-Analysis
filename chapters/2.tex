\section{Clustering Analysis by K-means} % (fold)
\label{sec:clustering_analysis_by_k_means}
\subsection{Choice of attributes and distance function} % (fold)
\label{sub:choice_of_attributes_and_distance_function}
\begin{wrapfigure}{r}{7cm}
    \begin{center}
        \subcaptionbox{\label{fig:sse_left}}{\includegraphics[scale=0.45]{images/kmeans/SSE_left.pdf}}
        \subcaptionbox{\label{fig:sse_stayed}}{\includegraphics[scale=0.45]{images/kmeans/SSE_stayed.pdf}}
        \caption{Nella Figura \ref{fig:sse_left} troviamo la rappresentazione della curva con cui l'SSE dei
        cluster relativi ai dipendenti che hanno lasciato l'azienda decresce con l'aumentare del numero di cluster
        . Lo stesso vale per la Figura \ref{fig:sse_stayed}, ma per i dipendenti che sono ancora all'interno
        dell'azienda.}
    \end{center}
\end{wrapfigure}
Le variabili sulle quali abbiamo deciso di applicare la Cluster Analysis tramite K-means sono le due
variabili di tipo continous presenti nel Dataset, ossia \textit{Satisfaction Level} e \textit{Latest Evaluation}. Le
variabili di tipo categorico sono state scartate al momento della scelta dato che la natura stessa dell'algoritmo
prevede il suo utilizzo su variabili di tipo numerico. \\ \\ Approfondire su variabili discrete \\ \\
Nell'implementazione dell'algoritmo da noi utilizzata è stato deciso di applicare la distanza euclidea come distance
function.
% subsection choice_of_attributes_and_distance_function (end)
\subsection{Identification of the best value of k} % (fold)
\label{sub:identification_of_the_best_value_of_k}
Al fine di identificare il miglior numero $k$ di clusters da utilizzare, abbiamo tenuto conto dell'Error Sum of
Squares (SSE) per ogni iterazione dell'algoritmo, svolta a partire da un valore iniziale di $k$ pari a $2$ fino
ad un valore massimo di $50$. Rappresentato in Figura \ref{fig:sse_left} troviamo l'andamento dell'SSE per i
cluster relativi ai dipendenti che hanno lasciato l'azienda. Possiamo notare il valore ottimale di $k$ pari a $3$,
che è la posizione sull'asse dei cluster dove la curva inizia il suo percorso discendente. In figura
\ref{fig:sse_stayed} possiamo vedere la stessa cosa, ma per i dipendenti che sono rimasti all'interno
dell'azienda. In questo caso notiamo il valore ottimale di $k$ pari a $5$.
% subsection identification_of_the_best_value_of_k (end)
\subsection{Characterization of the obtained clusters } % (fold)
\label{sub:characterization_of_the_obtained_clusters}
Andiamo adesso a dare una descrizione dei cluster emersi dall'applicazione dell'algoritmo K-means con i parametri
che abbiamo deciso di utilizzare. Cominciando con l'osservare lo scatterplot relativo ai cluster degli impiegati
che hanno lasciato l'azienda, Figura , notiamo che sono ben visibili $3$ categorie distinte di impiegati, che
possiamo descrivere discorsivamente come
\begin{itemize}
    \item Impiegati con un livello di soddisfazione basso e un alto score nella valutazione. Questo cluster ci
    suggerisce la motivazione per cui questi impiegati hanno lasciato l'azienda, ossia una quantità troppo elevata
    di ore di lavoro, dalla quale deriva probabilmente l'alto score nella valutazione.
    \item Impiegati con un basso livello di soddisfazione, compreso tra $0.3$ e $0.5$, e un altrettanto basso score
    nella valutazione, compreso tra $0.2$ e $0.55$. La scarsa produttività e soddisfazione suggeriscono che questo
    gruppo di impiegati ha deciso di lasciare l'azienda per insoddisfazione verso la posizione lavorativa offerta.
    \item Impiegati con un alto livello di soddisfazione e un alto score nella valutazione, che probabilmente hanno
    lasciato l'azienda dopo aver ricevuto un'offerta di lavoro più vantaggiosa.
\end{itemize}
\lipsum
\begin{figure}[t]
    \begin{center}
        \subcaptionbox{\label{fig:cluster_left}}{\includegraphics[scale=0.45]{images/kmeans/cluster_left.pdf}}
        \subcaptionbox{\label{fig:cluster_stayed}}{\includegraphics[scale=0.45]{images/kmeans/cluster_stayed.pdf}}
        \caption{Nella Figura \ref{fig:cluster_left} troviamo i cluster emersi dall'analisi dei dipendenti che
        hanno lasciato l'azienda, nella Figura \ref{fig:cluster_stayed} troviamo invece quelli emersi dall'analisi
        dei dipendenti rimasti nell'azienda.}
    \end{center}
\end{figure}
% subsection characterization_of_the_obtained_clusters (end)
% section clustering_analysis_by_k_means (end)


\newpage
\section{Hierarchical clustering}
Dire all'inizio del clustering quali variabili si utilizzano e normalizzazione. Sulle variabili numeriche del dataset è stato eseguito un clustering gerarchico di tipo agglomerativo, con l'utilizzo di diverse metriche e differenti metodi per il calcolo della dissimilarità tra clusters. Il confronto tra i diversi metodi è stato effettuato utilizzando come indice di validità (non supervisionato), la \textit{cophenetic correlation} $\rho_{coph}$. Ciascuna coppia di dati del dataset è prima o poi unita nel dendrogramma in un nodo, ad una distanza denominata \textit{cophenetic distance}. La cophenetic correlation è calcolata come coefficiente di correlazione di Pearson tra gli $n(n-1)/2$ valori della matrice delle distanze e le corrispondenti cophenetic distances. Il valore di tale correlazione può essere interpretato come il grado di aderenza del dendrogramma alle distanze originali, ed è quindi una misura della sua validità.
In figura \ref{fig:hierarchical-comparison} si osserva che tra i metodi utilizzati \textit{average} e \textit{centroid} risultano quelli con il valore massimo della cophenetic correlation ed equivalenti tra loro. DESCRIVERE METODI Per ciascun metodo sono state testate tre metriche, quella euclidea, Manhattan e di Mahalanobis. Quest'ultima è una distanza non isotropa che prende in considerazione la correlazione tra le variabili. Per i due metodi migliori la distanza euclidea risulta lievemente più efficace, anche se la differenza tra le metriche non è così rilevante. Questo confronto ha quindi permesso una prima scrematura dei vari metodi, da cui sono stati ottenuti 6 differenti dendrogrammi.
Osservando la figura REF si osserva che i dendrogrammi sono nettamente diversi tra loro anche a parità di $\rho_{coph}$, il clustering gerarchico risulta dunque molto sensibile al metodo e alla metrica utilizzata nell'algoritmo. E' necessario dunque utilizzare un ulteriore indice per confrontare i dendrogrammi.

\todo[inline]{Figure da sistemare per dimensioni/quantità}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../images/hierarchical/methods_comparison.pdf}
  \caption{Confronto tra diversi metodi}
  \label{fig:hierarchical-comparison}
\end{figure}



\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{../images/hierarchical/silhouette_comparison.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}



\begin{figure}[htbp]
  \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_euclidean.pdf}
        \caption{dendrogramma X}

    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_centroid_euclidean.pdf}

      \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
    \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_mahalanobis.pdf}

    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
    \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_centroid_mahalanobis.pdf}

    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
    \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_cityblock.pdf}
        \caption{Distribuzione relativa alla variabile \textit{Work Accident}}

    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      
    \begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_centroid_cityblock.pdf}
    \end{subfigure}
\caption{Dendrogrammi con massima \textit{cophenetic correlation}}
  \end{figure}

  \begin{comment}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical/dendrogram_average_euclidean.pdf}
  \caption{Dendrogramma per method X e metrica Y}
\end{figure}
\end{comment}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{../images/hierarchical/silhouette_average_euclidean_n2.pdf}
  \caption{Confronto tra silhouette medie, per due clusters}
\end{figure}



