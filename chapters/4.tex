Al fine di costruire un \textit{decision tree} con i record a nostra disposizione, abbiamo eseguito alcune
manipolazioni prima della costruzione dell'albero vero e proprio: sia la variabile \textit{Salary} che la
variabile \textit{Department} sono state convertite in variabili numeriche, per facilitare l'applicazione dell'
algoritmo sui records, inoltre, per fornire un \textit{target} durante la costruzione dell'albero, la variabile
\textit{Left} è stata separata dal data set fornito. Tutte le altre variabili sono state lasciate inalterate.
\section{Learning of different decision trees} % (fold)
\label{sec:learning_of_different_decision_trees}
Durante la fase di costruzione vera e propia, abbiamo deciso di implementare due alberi distinti, differenziandoli
nel modo in cui essi avrebbero dovuto scegliere lo \textit{split} migliore. Abbiamo perciò costruito un albero
basato sull'indice \textit{Gini} come misura di impurità di un nodo, e un albero basato sull'uso
dell'\textit{Entropia}. Inizialmente abbiamo costruito entrambi gli alberi senza imporre un limite sul numero di
samples che un nodo avrebbe dovuto contenere, successivamente però, per evitare di incorrere nel noto fenomeno
dell'\textit{overfitting}, abbiamo voluto esplorare le possibilità date dal consentire uno split solo se almeno il
$10\%$ o il $20\%$ rispetto al numero totale di data objects
% section learning_of_different_decision_trees (end)
\section{Decision trees interpretation} % (fold)
\label{sec:decision_trees_interpretation}

% section decision_trees_interpretation (end)
\section{Decision trees validation with test and training set} % (fold)
\label{sec:decision_trees_validation_with_test_and_training_set}

% section decision_trees_validation_with_test_and_training_set (end)
\section{Discussion of the best prediction model} % (fold)
\label{sec:discussion_of_the_best_prediction_model}

% section discussion_of_the_best_prediction_model (end)
